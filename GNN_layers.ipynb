{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, os.path\n",
    "\n",
    "os.environ['KMP_DUPLICATE_LIB_OK'] = 'True'\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import time\n",
    "import random\n",
    "import numpy as np\n",
    "import math\n",
    "import matplotlib\n",
    "import typing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_activation_function(activation_fn_name: str):\n",
    "    \"\"\"Convert from an activation function name to the function itself.\"\"\"\n",
    "    if activation_fn_name is None:\n",
    "        return None\n",
    "    activation_fn_name = activation_fn_name.lower()\n",
    "\n",
    "    string_to_activation_fn = {\n",
    "        \"linear\": None,\n",
    "        \"tanh\": tf.nn.tanh,\n",
    "        \"relu\": tf.nn.relu,\n",
    "        \"leaky_relu\": tf.nn.leaky_relu,\n",
    "        \"elu\": tf.nn.elu,\n",
    "        \"selu\": tf.nn.selu,\n",
    "        \"sigmoid\": tf.nn.sigmoid\n",
    "    }\n",
    "    activation_fn = string_to_activation_fn.get(activation_fn_name)\n",
    "    if activation_fn is None:\n",
    "        raise ValueError(f\"Unknown activation function: {activation_fn_name}\")\n",
    "    return activation_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def degree_matrix(matrix):\n",
    "    n = matrix.shape[0]\n",
    "    degree_m = np.zeros((n, n))\n",
    "    for i in range(n):\n",
    "        degree_m[i][i] = matrix[i].sum()\n",
    "    return degree_m\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Basic Graph Convolutional layer\n",
    "\"\"\"\n",
    "\n",
    "class GCNLayer(layers.Layer):\n",
    "    def __init__(self, output_dim, activation=\"linear\", **kwargs):\n",
    "        self.output_dim = output_dim\n",
    "        self.activation = get_activation_function(activation)\n",
    "        super(GCNLayer, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        A_shape, H_shape = input_shape\n",
    "        self.w = self.add_weight(name=\"weights\", shape=[H_shape[-1], self.output_dim])\n",
    "        self.b = self.add_weight(name=\"bias\", shape=[self.output_dim])\n",
    "\n",
    "        super(GCNLayer, self).build(input_shape)\n",
    "\n",
    "    def call(self, inputs, **kwargs):\n",
    "        print(inputs)\n",
    "        A, H = inputs[0], inputs[1]\n",
    "        if self.activation != \"None\":\n",
    "            return self.activation(tf.matmul(tf.matmul(A, H), self.w) + self.b)\n",
    "        return (tf.matmul(tf.matmul(A, H), self.w) + self.b)\n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Gated Graph Cell\n",
    "Input is a 2n x n adjacency matrix in addition to the initial embedding. 2n as in the paper they split up edges \n",
    "in non directed graphs into two directed edges. Might just add calculating the second matrix into the call part of the \n",
    "fn so that we dont need to write out seperate adjacency matrices \n",
    "\n",
    "\n",
    "propogation_no is the number of propogation steps\n",
    "Hopefull that this is the right implementation\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "class GGNNcell(layers.Layer):\n",
    "    def __init__(self, propogation_no, activation=\"sigmoid\", **kwargs):\n",
    "        self.activation = get_activation_function(activation)\n",
    "        self.propogation_no = propogation_no\n",
    "        super(GGNNcell, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        A_shape, H_shape = input_shape\n",
    "\n",
    "        self.bias = self.add_weight(name=\"bias\", shape=[A_shape[-1], H_shape[-1]])\n",
    "\n",
    "        self.w_reset = self.add_weight(name=\"w_reset\", shape=[A_shape[-1], A_shape[-2]])\n",
    "        self.u_reset = self.add_weight(name=\"u_reset\", shape=[A_shape[-1], A_shape[-1]])\n",
    "\n",
    "        self.w_update = self.add_weight(name=\"w_update\", shape=[A_shape[-1], A_shape[-2]])\n",
    "        self.u_update = self.add_weight(name=\"u_update\", shape=[A_shape[-1], A_shape[-1]])\n",
    "\n",
    "        self.w = self.add_weight(name=\"w\", shape=[A_shape[-1], A_shape[-2]])\n",
    "        self.u = self.add_weight(name=\"u\", shape=[A_shape[-1], A_shape[-1]])\n",
    "\n",
    "    def call(self, inputs):\n",
    "        A, H = inputs[0], inputs[1]\n",
    "        initial_code = inputs[1]\n",
    "        for i in range(self.propogation_no):\n",
    "            a = tf.matmul(A, H) + self.bias\n",
    "\n",
    "            reset = self.activation(tf.matmul(self.w_reset, a) + tf.matmul(self.u_reset, H))\n",
    "            update = self.activation(tf.matmul(self.w_update, a) + tf.matmul(self.u_update, H))\n",
    "\n",
    "            h_update = tf.nn.tanh(tf.matmul(self.w, a) + tf.matmul(self.u, (tf.math.multiply(reset, H))))\n",
    "            h_update = tf.math.multiply(1 - update, H) + tf.math.multiply(update, h_update)\n",
    "\n",
    "            H = h_update\n",
    "\n",
    "        return A, H"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Graph level output which involves two networks taking in both initial embeddings and propogated embeddings\n",
    "then taking the dot product per vertex of the two networks output, then summing them together and finally applying \n",
    "some sigmoid fn. Dot product apparently is acting as a sort of attention mechanism, but will read up on that. \n",
    "\"\"\"\n",
    "\n",
    "\"still need to implement the node level output\"\n",
    "\n",
    "class GGNN_Graph_out(layers.Layer):\n",
    "    def __init__(self, propogatino_no, output_dim, **kwargs):\n",
    "        self.GGNN_cell = GGNNcell(propogatino_no)\n",
    "        self.network_i = layers.Dense(output_dim, actvation = 'sigmoid ')\n",
    "        self.network_j = layers.Dense(output_dim, activation = 'tanh')\n",
    "        super(GGNN_Graph_out, self).__init__(**kwargs)\n",
    "\n",
    "\n",
    "    def call(self, inputs):\n",
    "        A, H = inputs[0], inputs[1]\n",
    "        prop_out = self.GGNN_cell(A, H)[1]\n",
    "        a = self.network_i((prop_out, H))\n",
    "        b = self.network_j((prop_out, H))\n",
    "        dot = tf.math.multiply(a,b)\n",
    "        dot = tf.nn.tanh(tf.math.reduce_sum(dot, axis = 0))\n",
    "        return dot\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(<tf.Tensor: id=36, shape=(3, 3), dtype=float32, numpy=\n",
      "array([[1., 1., 1.],\n",
      "       [1., 1., 0.],\n",
      "       [1., 0., 1.]], dtype=float32)>, <tf.Tensor: id=37, shape=(3, 4), dtype=float32, numpy=\n",
      "array([[1., 0., 0., 0.],\n",
      "       [1., 0., 0., 0.],\n",
      "       [1., 0., 0., 0.]], dtype=float32)>)\n",
      "tf.Tensor(\n",
      "[[-0.9987618  -0.9879896 ]\n",
      " [-0.9922957  -0.93105525]\n",
      " [-0.9922957  -0.93105525]], shape=(3, 2), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "graph_layer = GGNNcell(1)\n",
    "graph_layer = GCNLayer(2, activation=\"tanh\")\n",
    "graph = np.asarray([[1, 1, 1], [1, 1, 0], [1, 0, 1]]).astype('float32')\n",
    "embedding = np.asarray([[1, 0, 0, 0], [1, 0, 0, 0], [1, 0, 0, 0]]).astype('float32')\n",
    "\n",
    "print(graph_layer((graph, embedding)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
